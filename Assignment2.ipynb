{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, Subset\nimport numpy as np\nfrom sklearn.utils import shuffle # for shuffling\nimport os\nimport cv2\nimport random\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-04T09:49:05.157906Z","iopub.execute_input":"2024-04-04T09:49:05.158337Z","iopub.status.idle":"2024-04-04T09:49:14.828473Z","shell.execute_reply.started":"2024-04-04T09:49:05.158304Z","shell.execute_reply":"2024-04-04T09:49:14.827603Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# !pip install wandb\nimport wandb\n# !wandb login\nwandb.login(key=\"338ff25d87248e7f3e86e98c746e32fe09553c9e\")\n\n#338ff25d87248e7f3e86e98c746e32fe09553c9e","metadata":{"execution":{"iopub.status.busy":"2024-04-04T09:49:19.416787Z","iopub.execute_input":"2024-04-04T09:49:19.417284Z","iopub.status.idle":"2024-04-04T09:49:21.818732Z","shell.execute_reply.started":"2024-04-04T09:49:19.417231Z","shell.execute_reply":"2024-04-04T09:49:21.817881Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import gc","metadata":{"execution":{"iopub.status.busy":"2024-04-04T09:49:21.820420Z","iopub.execute_input":"2024-04-04T09:49:21.820873Z","iopub.status.idle":"2024-04-04T09:49:21.825205Z","shell.execute_reply.started":"2024-04-04T09:49:21.820830Z","shell.execute_reply":"2024-04-04T09:49:21.824141Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!wget https://storage.googleapis.com/wandb_datasets/nature_12K.zip -O nature_12K.zip\n!unzip -q nature_12K.zip","metadata":{"execution":{"iopub.status.busy":"2024-04-04T09:49:21.826614Z","iopub.execute_input":"2024-04-04T09:49:21.826913Z","iopub.status.idle":"2024-04-04T09:50:04.547452Z","shell.execute_reply.started":"2024-04-04T09:49:21.826890Z","shell.execute_reply":"2024-04-04T09:50:04.546348Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"--2024-04-04 09:49:22--  https://storage.googleapis.com/wandb_datasets/nature_12K.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 74.125.201.207, 74.125.69.207, 64.233.181.207, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|74.125.201.207|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3816687935 (3.6G) [application/zip]\nSaving to: 'nature_12K.zip'\n\nnature_12K.zip      100%[===================>]   3.55G   265MB/s    in 14s     \n\n2024-04-04 09:49:36 (261 MB/s) - 'nature_12K.zip' saved [3816687935/3816687935]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm nature_12K.zip","metadata":{"execution":{"iopub.status.busy":"2024-04-04T09:50:15.609708Z","iopub.execute_input":"2024-04-04T09:50:15.610112Z","iopub.status.idle":"2024-04-04T09:50:17.164960Z","shell.execute_reply.started":"2024-04-04T09:50:15.610069Z","shell.execute_reply":"2024-04-04T09:50:17.163789Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"dtype = torch.float\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2024-04-04T09:50:20.205379Z","iopub.execute_input":"2024-04-04T09:50:20.206105Z","iopub.status.idle":"2024-04-04T09:50:20.258560Z","shell.execute_reply.started":"2024-04-04T09:50:20.206071Z","shell.execute_reply":"2024-04-04T09:50:20.257319Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-04T09:50:21.257122Z","iopub.execute_input":"2024-04-04T09:50:21.257872Z","iopub.status.idle":"2024-04-04T09:50:21.262720Z","shell.execute_reply.started":"2024-04-04T09:50:21.257839Z","shell.execute_reply":"2024-04-04T09:50:21.261757Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"resize_width = 500\nresize_height= 500","metadata":{"execution":{"iopub.status.busy":"2024-04-04T09:50:21.759624Z","iopub.execute_input":"2024-04-04T09:50:21.760496Z","iopub.status.idle":"2024-04-04T09:50:23.047916Z","shell.execute_reply.started":"2024-04-04T09:50:21.760461Z","shell.execute_reply":"2024-04-04T09:50:23.046801Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def getDimOfLastConv(num_filters, filter_sizes, use_batch_norm=False):\n    layers = []\n    \n    # Initial convolution layer\n    layers.append(nn.Conv2d(3, num_filters[0], kernel_size=filter_sizes[0], stride=1, padding=0))\n    layers.append(nn.ReLU())\n    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n    if use_batch_norm:\n        layers.append(nn.BatchNorm2d(num_filters[0]))\n    \n    # Subsequent convolution layers\n    for i in range(1, len(num_filters)):\n        layers.append(nn.Conv2d(num_filters[i-1], num_filters[i], kernel_size=filter_sizes[i], stride=1, padding=0))\n        layers.append(nn.ReLU())\n        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n        if use_batch_norm:\n            layers.append(nn.BatchNorm2d(num_filters[i]))\n    \n    conv_stack = nn.Sequential(*layers)\n    \n    image_tensor = torch.zeros([1, 3, resize_width, resize_height])\n    x = conv_stack(image_tensor)\n    flat = nn.Flatten()\n    x = flat(x)\n    return x.shape[-1]","metadata":{"execution":{"iopub.status.busy":"2024-04-04T09:50:23.050135Z","iopub.execute_input":"2024-04-04T09:50:23.050549Z","iopub.status.idle":"2024-04-04T09:50:23.823520Z","shell.execute_reply.started":"2024-04-04T09:50:23.050515Z","shell.execute_reply":"2024-04-04T09:50:23.822567Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class SimpleCNN(nn.Module):\n  def __init__(self, num_filters, filter_sizes, activation_fn, num_neurons_dense, use_batch_norm, dropout_prob):\n    super(SimpleCNN, self).__init__()\n\n    layers = []\n    \n    # Initial convolution layer\n    layers.append(nn.Conv2d(3, num_filters[0], kernel_size=filter_sizes[0], stride=1, padding=0))\n    layers.append(activation_fn)\n    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n    if use_batch_norm:\n        layers.append(nn.BatchNorm2d(num_filters[0]))\n    \n    # Subsequent convolution layers\n    for i in range(1, len(num_filters)):\n        layers.append(nn.Conv2d(num_filters[i-1], num_filters[i], kernel_size=filter_sizes[i], stride=1, padding=0))\n        layers.append(activation_fn)\n        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n        if use_batch_norm:\n            layers.append(nn.BatchNorm2d(num_filters[i]))\n    \n    self.conv_stack = nn.Sequential(*layers)\n\n    flattenNodes = getDimOfLastConv(num_filters, filter_sizes, use_batch_norm)\n    self.flatten = nn.Flatten()\n    self.dense = nn.Linear(flattenNodes, num_neurons_dense)\n    self.dropout = nn.Dropout(dropout_prob)\n    self.output = nn.Linear(num_neurons_dense, 10)\n\n  def forward(self, x):\n    x = self.conv_stack(x)  # Pass input through conv_stack\n    x = self.flatten(x)  # Flatten the output of conv layers\n    x = self.dense(x) # passing through dense layer\n    x = self.output(x)\n    return x\n","metadata":{"execution":{"iopub.status.busy":"2024-04-04T09:50:23.824826Z","iopub.execute_input":"2024-04-04T09:50:23.825189Z","iopub.status.idle":"2024-04-04T09:50:23.836568Z","shell.execute_reply.started":"2024-04-04T09:50:23.825155Z","shell.execute_reply":"2024-04-04T09:50:23.835677Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# # Defining your transformations\n# transform = transforms.Compose([\n#     transforms.Resize((resize_width, resize_height)),  # Resize to 256x256\n#     transforms.ToTensor(),\n#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n# ])\n\n# # Dataset\n# TrainDataset = datasets.ImageFolder(root='inaturalist_12K/train', transform=transform)\n# # DataLoader with shuffling\n# TrainData_loader = DataLoader(TrainDataset,shuffle=True,num_workers=2,batch_size=64,pin_memory=True)\n\n# # same for validation\n# ValidationDataset = datasets.ImageFolder(root='inaturalist_12K/val', transform=transform)\n# # DataLoader with shuffling\n# ValidationData_loader = DataLoader(ValidationDataset, batch_size=64)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-04T09:50:24.506333Z","iopub.execute_input":"2024-04-04T09:50:24.506721Z","iopub.status.idle":"2024-04-04T09:50:24.511374Z","shell.execute_reply.started":"2024-04-04T09:50:24.506694Z","shell.execute_reply":"2024-04-04T09:50:24.510310Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def load_data(train_dir, test_dir, batchSize):\n    \n    # Transformation\n    transform = transforms.Compose([\n        transforms.Resize((resize_width, resize_height)),  # Resize to 256x256\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n    ])\n\n    # Dataset\n    TrainDataset = datasets.ImageFolder(root=train_dir, transform=transform)\n    class_to_idx = TrainDataset.class_to_idx\n\n    # Initialize lists to hold indices for training and validation\n    train_indices = []\n    val_indices = []\n\n    # Split indices for each class\n    for class_name, class_index in class_to_idx.items():\n        # Find indices of images in the current class\n        class_indices = [i for i, (_, label) in enumerate(TrainDataset.samples) if label == class_index]\n\n        # Split these indices into training and validation\n        _train_indices, _val_indices = train_test_split(class_indices, test_size=0.2, random_state=42)\n\n        # Append to the main list\n        train_indices.extend(_train_indices)\n        val_indices.extend(_val_indices)\n\n    # Create subsets for training and validation\n    train_subset = Subset(TrainDataset, train_indices)\n    val_subset = Subset(TrainDataset, val_indices)\n\n    # Create data loaders\n    train_loader = DataLoader(train_subset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_subset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=True)\n\n    # same for validation\n    TestDataset = datasets.ImageFolder(root=test_dir, transform=transform)\n    # DataLoader with shuffling\n    TestData_loader = DataLoader(TestDataset,num_workers=2, batch_size=batchSize, pin_memory=True)\n    \n    return train_loader, val_loader, TestData_loader\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-04T09:50:25.115302Z","iopub.execute_input":"2024-04-04T09:50:25.115966Z","iopub.status.idle":"2024-04-04T09:50:25.126322Z","shell.execute_reply.started":"2024-04-04T09:50:25.115937Z","shell.execute_reply":"2024-04-04T09:50:25.125420Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"\ndef train(model, criterion, optimizer, num_epochs, train_loader, val_loader):\n    for epoch in range(num_epochs):\n        model.train()\n#         for inputs, labels in train_loader:\n        for ind, (inputs, labels) in enumerate(tqdm(train_loader, desc=f'Training Progress {epoch+1}')):\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        find_accuracy(model, criterion, train_loader, \"train\")\n        find_accuracy(model, criterion, val_loader, \"validation\")\n\ndef find_accuracy(model, criterion, dataLoader, dataName):\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, labels in dataLoader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            val_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    \n    print(f'{dataName} Loss: {val_loss/len(dataLoader)}, '\n          f'{dataName} Accuracy: {100*correct/total}%\\n')\n    \n#     wandb.log({f'{dataName}_accuracy': 100*correct/total})\n#     wandb.log({f'{dataName}_loss': val_loss/len(dataLoader)})\n    \ndef train_model(learning_rate, num_filters, filter_sizes, activation_fn, optimiser_fn, num_neurons_dense, weight_decay, dropout, useBatchNorm, batchSize, num_epochs):\n    activation_dict = {'relu': nn.ReLU(), 'elu': nn.ELU(), 'selu': nn.SELU()}\n    \n    trainDataLoader, valDataLoader, testDataLoader = load_data(train_dir = 'inaturalist_12K/train', test_dir = 'inaturalist_12K/val', batchSize = batchSize)\n    \n#     num_epochs = 10\n    model = SimpleCNN(num_filters=num_filters, filter_sizes=filter_sizes, \n                  activation_fn=activation_dict[activation_fn], num_neurons_dense=num_neurons_dense,\n                  dropout_prob=dropout, use_batch_norm = useBatchNorm)\n\n    criterion = nn.CrossEntropyLoss()\n    if optimiser_fn == \"adam\":\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    elif optimiser_fn == \"nadam\":\n        optimizer = optim.NAdam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    elif optimiser_fn == \"rmsprop\":\n        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    else:\n        # stocastic gradient decent        \n        optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = torch.nn.DataParallel(model,device_ids = [0,1]).to(device)\n\n\n    train(model, criterion, optimizer, num_epochs, trainDataLoader, valDataLoader)\n    \n    return model\n    #this for the sweeps\n    #clearing the ram\n    model.cpu()\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-04T09:52:10.066471Z","iopub.execute_input":"2024-04-04T09:52:10.066812Z","iopub.status.idle":"2024-04-04T09:52:10.083610Z","shell.execute_reply.started":"2024-04-04T09:52:10.066788Z","shell.execute_reply":"2024-04-04T09:52:10.082533Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"model = train_model(learning_rate = 1e-3, num_filters = [32,32,32,32,32], filter_sizes=[3,3,3,3,3], activation_fn = \"relu\", optimiser_fn =\"nadam\", num_neurons_dense = 256, weight_decay = 0.0005, dropout = 0.2, useBatchNorm = False, batchSize = 32, num_epochs = 1)\n   ","metadata":{"execution":{"iopub.status.busy":"2024-04-04T09:52:11.079030Z","iopub.execute_input":"2024-04-04T09:52:11.079563Z","iopub.status.idle":"2024-04-04T09:55:11.729966Z","shell.execute_reply.started":"2024-04-04T09:52:11.079516Z","shell.execute_reply":"2024-04-04T09:55:11.728583Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Training Progress 1: 100%|██████████| 250/250 [01:22<00:00,  3.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 2.0788727478981017, train Accuracy: 25.62820352544068%\n\nvalidation Loss: 2.083382318890284, validation Accuracy: 25.35%\n\n","output_type":"stream"}]},{"cell_type":"code","source":"trainDataLoader, valDataLoader, testDataLoader = load_data(train_dir = 'inaturalist_12K/train', test_dir = 'inaturalist_12K/val', batchSize = 16)\n    \ncriterion = nn.CrossEntropyLoss()\nfind_accuracy(model, criterion, valDataLoader, \"val\")\nfind_accuracy(model, criterion, testDataLoader, \"test\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n    wandb.init(project=\"Assignment 2\")\n    config = wandb.config\n    run_name = f\"{config.optimiser}_{config.activation}_{config.num_filters}_{config.batch_size}\"\n\n    # Set the run name\n    wandb.run.name = run_name\n    wandb.run.save()\n\n    # Define and train the model as before\n    train_model(learning_rate = config.learning_rate, num_filters = config.num_filters,\n                filter_sizes = config.filter_sizes, activation_fn = config.activation, \n                optimiser_fn = config.optimiser, num_neurons_dense = config.dense_layer,\n                weight_decay = config.weight_decay, dropout = config.dropout, useBatchNorm = False, \n                batchSize = config.batch_size, num_epochs = 10)\n    \nsweep_config = {\n    'method': 'bayes',\n    'name' : 'sweep cross entropy',\n    'metric': {\n      'name': 'validation_accuracy',\n      'goal': 'maximize'\n    },\n    'parameters': {\n        'num_filters': {\n          'values': [[32,32,32,32,32],[32,64,64,128,128],[128,128,64,64,32],[32,64,128,256,512]]\n        },\n        'filter_sizes': {\n          'values': [[3,3,3,3,3], [5,5,5,5,5], [3,5,3,5,3]]\n        },\n        'weight_decay': {\n            'values':[0, 0.0005, 0.5]\n        },\n        'learning_rate': {\n            'values':[1e-3,1e-4]\n        },\n        'weight_decay': {\n            'values': [0, 0.0005, 0.005]\n        },\n        'dropout': {\n            'values': [0, 0.2, 0.4]\n        },\n        'learning_rate': {\n            'values': [1e-3, 1e-4]\n        },\n        'activation': {\n            'values': ['relu', 'elu', 'selu']\n        },\n        'optimiser': {\n            'values': ['nadam', 'adam', 'rmsprop']\n        },\n        'batch_norm':{\n            'values': ['true','false']\n        },\n        'batch_size': {\n            'values': [32, 64]\n        },\n        'dense_layer':{\n            'values': [128, 256, 512]\n        }\n    }\n}\n\n\nsweep_id = wandb.sweep(sweep=sweep_config,project='Assignment 2')\nwandb.agent(\"k4kg156v\" , function = main , count = 400)\n\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-04-04T09:48:56.555696Z","iopub.status.idle":"2024-04-04T09:48:56.556077Z","shell.execute_reply.started":"2024-04-04T09:48:56.555878Z","shell.execute_reply":"2024-04-04T09:48:56.555893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}