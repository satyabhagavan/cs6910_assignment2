{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data.dataloader import DataLoader\nimport numpy as np\nfrom sklearn.utils import shuffle # for shuffling\nimport os\nimport cv2\nimport random\nimport torch.nn as nn\nimport torch.optim as optim","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-01T16:50:15.977681Z","iopub.execute_input":"2024-04-01T16:50:15.978271Z","iopub.status.idle":"2024-04-01T16:50:24.332162Z","shell.execute_reply.started":"2024-04-01T16:50:15.978216Z","shell.execute_reply":"2024-04-01T16:50:24.331409Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:50:24.333756Z","iopub.execute_input":"2024-04-01T16:50:24.334539Z","iopub.status.idle":"2024-04-01T16:50:24.338525Z","shell.execute_reply.started":"2024-04-01T16:50:24.334504Z","shell.execute_reply":"2024-04-01T16:50:24.337690Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import gc","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:50:35.619280Z","iopub.execute_input":"2024-04-01T16:50:35.619892Z","iopub.status.idle":"2024-04-01T16:50:35.623786Z","shell.execute_reply.started":"2024-04-01T16:50:35.619859Z","shell.execute_reply":"2024-04-01T16:50:35.622834Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!wget https://storage.googleapis.com/wandb_datasets/nature_12K.zip -O nature_12K.zip\n!unzip -q nature_12K.zip","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:50:36.800887Z","iopub.execute_input":"2024-04-01T16:50:36.801708Z","iopub.status.idle":"2024-04-01T16:52:32.786089Z","shell.execute_reply.started":"2024-04-01T16:50:36.801676Z","shell.execute_reply":"2024-04-01T16:52:32.784916Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"--2024-04-01 16:50:37--  https://storage.googleapis.com/wandb_datasets/nature_12K.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 173.194.69.207, 108.177.96.207, 108.177.119.207, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|173.194.69.207|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3816687935 (3.6G) [application/zip]\nSaving to: 'nature_12K.zip'\n\nnature_12K.zip      100%[===================>]   3.55G  42.2MB/s    in 88s     \n\n2024-04-01 16:52:05 (41.4 MB/s) - 'nature_12K.zip' saved [3816687935/3816687935]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm nature_12K.zip","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:52:32.788137Z","iopub.execute_input":"2024-04-01T16:52:32.788458Z","iopub.status.idle":"2024-04-01T16:52:34.290505Z","shell.execute_reply.started":"2024-04-01T16:52:32.788430Z","shell.execute_reply":"2024-04-01T16:52:34.289272Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"dtype = torch.float\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:52:34.291867Z","iopub.execute_input":"2024-04-01T16:52:34.292158Z","iopub.status.idle":"2024-04-01T16:52:34.378537Z","shell.execute_reply.started":"2024-04-01T16:52:34.292132Z","shell.execute_reply":"2024-04-01T16:52:34.377416Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:52:34.380648Z","iopub.execute_input":"2024-04-01T16:52:34.380919Z","iopub.status.idle":"2024-04-01T16:52:35.568434Z","shell.execute_reply.started":"2024-04-01T16:52:34.380897Z","shell.execute_reply":"2024-04-01T16:52:35.567413Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"resize_width = 500\nresize_height= 500","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:52:35.569581Z","iopub.execute_input":"2024-04-01T16:52:35.569845Z","iopub.status.idle":"2024-04-01T16:52:37.237204Z","shell.execute_reply.started":"2024-04-01T16:52:35.569823Z","shell.execute_reply":"2024-04-01T16:52:37.236197Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Defining your transformations\ntransform = transforms.Compose([\n    transforms.Resize((resize_width, resize_height)),  # Resize to 256x256\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n])\n\n# Dataset\nTrainDataset = datasets.ImageFolder(root='inaturalist_12K/train', transform=transform)\n# DataLoader with shuffling\nTrainData_loader = DataLoader(TrainDataset,shuffle=True,num_workers=2,batch_size=64,pin_memory=True)\n\n# same for validation\nValidationDataset = datasets.ImageFolder(root='inaturalist_12K/val', transform=transform)\n# DataLoader with shuffling\nValidationData_loader = DataLoader(ValidationDataset, batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:52:37.238396Z","iopub.execute_input":"2024-04-01T16:52:37.238676Z","iopub.status.idle":"2024-04-01T16:52:38.615041Z","shell.execute_reply.started":"2024-04-01T16:52:37.238653Z","shell.execute_reply":"2024-04-01T16:52:38.614108Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"type(TrainData_loader)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:52:38.616372Z","iopub.execute_input":"2024-04-01T16:52:38.617104Z","iopub.status.idle":"2024-04-01T16:52:38.624076Z","shell.execute_reply.started":"2024-04-01T16:52:38.617069Z","shell.execute_reply":"2024-04-01T16:52:38.623204Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"torch.utils.data.dataloader.DataLoader"},"metadata":{}}]},{"cell_type":"code","source":"for batch in TrainData_loader:\n    for tensor in batch:\n        print(tensor.device)\n        break\n    break","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:52:38.625162Z","iopub.execute_input":"2024-04-01T16:52:38.625470Z","iopub.status.idle":"2024-04-01T16:52:41.472553Z","shell.execute_reply.started":"2024-04-01T16:52:38.625446Z","shell.execute_reply":"2024-04-01T16:52:41.471589Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"images , labels = next(iter(TrainData_loader))\nimages.size()","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:52:41.473941Z","iopub.execute_input":"2024-04-01T16:52:41.474264Z","iopub.status.idle":"2024-04-01T16:52:43.989213Z","shell.execute_reply.started":"2024-04-01T16:52:41.474202Z","shell.execute_reply":"2024-04-01T16:52:43.987899Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"torch.Size([64, 3, 500, 500])"},"metadata":{}}]},{"cell_type":"code","source":"def getDimOfLastConv(num_filters, filter_sizes, use_batch_norm=False):\n    layers = []\n    \n    # Initial convolution layer\n    layers.append(nn.Conv2d(3, num_filters[0], kernel_size=filter_sizes[0], stride=1, padding=0))\n    layers.append(nn.ReLU())\n    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n    if use_batch_norm:\n        layers.append(nn.BatchNorm2d(num_filters[0]))\n    \n    # Subsequent convolution layers\n    for i in range(1, len(num_filters)):\n        layers.append(nn.Conv2d(num_filters[i-1], num_filters[i], kernel_size=filter_sizes[i], stride=1, padding=0))\n        layers.append(nn.ReLU())\n        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n        if use_batch_norm:\n            layers.append(nn.BatchNorm2d(num_filters[i]))\n    \n    conv_stack = nn.Sequential(*layers)\n    \n    image_tensor = torch.zeros([1, 3, resize_width, resize_height])\n    x = conv_stack(image_tensor)\n    flat = nn.Flatten()\n    x = flat(x)\n    return x.shape[-1]","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:52:43.993055Z","iopub.execute_input":"2024-04-01T16:52:43.993401Z","iopub.status.idle":"2024-04-01T16:52:44.008143Z","shell.execute_reply.started":"2024-04-01T16:52:43.993372Z","shell.execute_reply":"2024-04-01T16:52:44.007267Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class SimpleCNN(nn.Module):\n  def __init__(self, num_filters, filter_sizes, activation_fn, num_neurons_dense, use_batch_norm = True):\n    super(SimpleCNN, self).__init__()\n\n    layers = []\n    \n    # Initial convolution layer\n    layers.append(nn.Conv2d(3, num_filters[0], kernel_size=filter_sizes[0], stride=1, padding=0))\n    layers.append(activation_fn)\n    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n    if use_batch_norm:\n        layers.append(nn.BatchNorm2d(num_filters[0]))\n    \n    # Subsequent convolution layers\n    for i in range(1, len(num_filters)):\n        layers.append(nn.Conv2d(num_filters[i-1], num_filters[i], kernel_size=filter_sizes[i], stride=1, padding=0))\n        layers.append(activation_fn)\n        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n        if use_batch_norm:\n            layers.append(nn.BatchNorm2d(num_filters[i]))\n    \n    self.conv_stack = nn.Sequential(*layers)\n\n    flattenNodes = getDimOfLastConv(num_filters, filter_sizes, use_batch_norm)\n    self.flatten = nn.Flatten()\n    self.dense = nn.Linear(flattenNodes, num_neurons_dense)\n    self.output = nn.Linear(num_neurons_dense, 10)\n\n  def forward(self, x):\n    x = self.conv_stack(x)  # Pass input through conv_stack\n    x = self.flatten(x)  # Flatten the output of conv layers\n    x = self.dense(x) # passing through dense layer\n    x = self.output(x)\n    return x\n","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:52:44.009275Z","iopub.execute_input":"2024-04-01T16:52:44.009589Z","iopub.status.idle":"2024-04-01T16:52:44.026594Z","shell.execute_reply.started":"2024-04-01T16:52:44.009555Z","shell.execute_reply":"2024-04-01T16:52:44.025640Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(learning_rate, num_filters, filter_sizes, activation_fn, num_neurons_dense, weight_decay):\n    \nnum_epochs = 10\nlearning_rate = 0.001\nnum_filters = [32, 32, 32, 32, 32]  \nfilter_sizes = [3, 3, 3, 3, 3]\nactivation_fn = nn.ReLU()\nnum_neurons_dense = 400\n\nmodel = SimpleCNN(num_filters=num_filters, filter_sizes=filter_sizes, \n                  activation_fn=activation_fn, num_neurons_dense=num_neurons_dense)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = torch.nn.DataParallel(model,device_ids = [0,1]).to(device)\n\ndef train(model, criterion, optimizer, num_epochs, train_loader, val_loader):\n    for epoch in range(num_epochs):\n        model.train()\n#         for inputs, labels in train_loader:\n        for ind, (inputs, labels) in enumerate(tqdm(train_loader, desc='Training Progress')):\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        find_accuracy(model, criterion, train_loader,\"train\")\n        find_accuracy(model, criterion, val_loader, \"validation\")\n\ndef find_accuracy(model, criterion, dataLoader, dataName):\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, labels in dataLoader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            val_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    \n    print(f'{dataName} Loss: {val_loss/len(dataLoader)}, '\n          f'{dataName} Accuracy: {100*correct/total}%\\n')\n\ntrain(model, criterion, optimizer, num_epochs, TrainData_loader, ValidationData_loader)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.cpu()\ndel model\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:59:38.520314Z","iopub.execute_input":"2024-04-01T16:59:38.520773Z","iopub.status.idle":"2024-04-01T16:59:38.745013Z","shell.execute_reply.started":"2024-04-01T16:59:38.520735Z","shell.execute_reply":"2024-04-01T16:59:38.744271Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"\nsweep_config = {\n    'method': 'bayes',\n    'name' : 'sweep cross entropy',\n    'metric': {\n      'name': 'v_accuracy',\n      'goal': 'maximize'\n    },\n    'parameters': {\n        'num_filters': {\n          'values': [[32,32,32,32,32],[32,64,64,128,128],[128,128,64,64,32],[32,64,128,256,512]]\n        },\n        'filter_sizes': {\n          'values': [[3,3,3,3,3], [5,5,5,5,5], [3,5,3,5,3]]\n        },\n        'weight_decay': {\n            'values':[0, 0.0005, 0.5]\n        },\n        'learning_rate': {\n            'values':[1e-3,1e-4]\n        },\n        'weight_decay': {\n            'values': [0, 0.0005, 0.005]\n        },\n        'dropout': {\n            'values': [0, 0.2, 0.4]\n        },\n        'learning_rate': {\n            'values': [1e-3, 1e-4]\n        },\n        'activation': {\n            'values': ['relu', 'elu', 'selu']\n        },\n        'optimiser': {\n            'values': ['nadam', 'adam', 'mgd']\n        }\n        'batch_norm':{\n            'values': ['true','false']\n        },\n        'data_augment': {\n            'values': ['true','false']\n        },\n        'batch_size': {\n            'values': [32, 64]\n        },\n        'dense_layer':{\n            'values': [64, 128, 256, 512]\n        }\n    }\n}","metadata":{},"execution_count":null,"outputs":[]}]}